<!DOCTYPE html>
<html>
  <head>
    <title>Getting Started with Agents Using LangChain</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

        class: center, middle

        # Getting Started with Agents Using LangChain
        ## By Lucas Soares
        ### 04/14/2025

        ---
        <div class="slide">
          <h1>Lucas Soares</h1>
          <div style="display: flex; align-items: center;">
            <ul style="flex: 1;">
              <li>AI Engineer</li>
              <br>
              <br>
              <br>
              <br>
            </ul>
            <img src="../notebooks/assets-resources/profile_pic.png" width="300" style="margin-left: 20px;">
          </div>
        </div>
        ---

        <div class="slide">
          <h1>Lucas Soares</h1>
          <div style="display: flex; align-items: center;">
            <ul style="flex: 1;">
              <li>AI Engineer</li>
              <br>
              <li>Instructor at O'Reilly Media</li>
              <br>
              <br>
            </ul>
            <img src="../notebooks/assets-resources/profile_pic.png" width="300" style="margin-left: 20px;">
          </div>
        </div>

        ---

        <div class="slide">
          <h1>Lucas Soares</h1>
          <div style="display: flex; align-items: center;">
            <ul style="flex: 1;">
              <li>AI Engineer</li>
              <br>
              <li>Instructor at O'Reilly Media</li>
              <br>
              <li>Curious about all things intelligence</li>
            </ul>
            <img src="../notebooks/assets-resources/profile_pic.png" width="300" style="margin-left: 20px;">
          </div>
        </div>

        ---

        # Table of Contents

        __1. Agents as Thought + Action__

        --

        __2. Defining Agents__

        --

        __3. Agents in 3 Levels of Complexity__

        --

        __4. OpenAI's Function API__

        --

        __5. LangChain Framework__

        --

        __6. Building Agents with LangChain__

        --

        __7. LangGraph__

        --

        __8. Concluding Remarks__

        --

        __9. References__

        ---

        # Thought + Action

        --

        - How do we do stuff? We __think__ and we __act__

        --

        - Example: Decision-making process for attending a live-training

        --

        <img style="width: 400px" src="../notebooks/assets-resources/thought_action.png">
        
        ---
        class: center, middle

        # Thinking: 
        
        
        ## What to do + planning (order, priority..)
        
        ---
        class: center, middle

        # Acting: 
        
        
        ## used __tools: search, browser, etc...__

        ---

        # What is an Agent? (LLM + Tool)
        
        <div style="display: flex; justify-content: space-around; margin-top: 2em;">
          <div style="text-align: center; width: 45%;">
            <h3 style="color: #0a0b0b;">LLM</h3>
            <p>Predicts next word/sentence</p>
            <img src="../notebooks/assets-resources/llm_predicts_pancakes.png" alt="LLM predicts pancakes" style="width: 100%; max-width: 300px; height: auto; margin-top: 1em;">
          </div>
          <div style="text-align: center; width: 45%;">
            <h3 style="color: #0a0b0b;">Tool</h3>
            <p>Performs actions in the real-world</p>
            <img src="../notebooks/assets-resources/pancake_maker.png" alt="Pancake maker" style="width: 100%; max-width: 300px; height: auto; margin-top: 1em;">
          </div>
        </div>


        ---
        
        # LLMs can use tools!

        - [Toolformer](https://arxiv.org/pdf/2302.04761.pdf)

        --

        <img style="width: 400px; margin-left:-5px "src="../notebooks/assets-resources/toolformer.png">

        <p style="font-size: 14px; margin-top: -10px;">
          <sup>[1]</sup> <a href="https://arxiv.org/pdf/2302.04761.pdf">(Schick u. a., o. J., 2023)</a>
        </p>

        ---
        # Interleaving Thoughts and Actions

        - [ReACT](https://arxiv.org/pdf/2210.03629.pdf): LLMs for __RE__asoning & __ACT__ion.

        <img style="width: 800px" src="../notebooks/assets-resources/react_paper_figure.png" >

        <p style="font-size: 14px; margin-top: -10px;">
          <sup>[2]</sup> <a href="https://arxiv.org/pdf/2210.03629.pdf">Yao, X., et al. (2023)</a>
        </p>

        ---
        
        ## 2025 Is the Year of Agents

        --

        <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin: 20px 0;">
          <div style="grid-column: span 2;">
            <img src="../notebooks/assets-resources/operator-release.png" style="width: 100%; height: 300px; object-fit: cover; border: 4px solid #4CAF50; box-shadow: 0 0 10px rgba(0,0,0,0.3);">
          </div>
          <img src="../notebooks/assets-resources/CleanShot 2025-01-24 at 15.30.19@2x.png" style="width: 100%; height: 150px; object-fit: contain;">
          <img src="../notebooks/assets-resources/CleanShot 2025-01-24 at 15.28.55@2x.png" style="width: 100%; height: 150px; object-fit: contain;">
          <img src="../notebooks/assets-resources/CleanShot 2025-01-24 at 15.32.23@2x.png" style="width: 100%; height: 150px; object-fit: contain;">
          <img src="../notebooks/assets-resources/CleanShot 2025-01-24 at 15.34.34@2x.png" style="width: 100%; height: 150px; object-fit: contain;" alt="Andrej Karpathy Tweet">
        </div>

        <!-- - [A Survey on Large Language Model based -->
        <!-- Autonomous Agents](https://arxiv.org/pdf/2308.11432.pdf)   -->

        

        <!-- <img style="width: 800px" src="../notebooks/assets-resources/agents_growth_trends.png"> -->
        <p style="font-size: 14px; margin-top: 10px;">Sources: <a href="https://www.axios.com/2025/01/23/davos-2025-ai-agents">Axios</a>, <a href="https://www.barrons.com/articles/nvidia-stock-ceo-ai-agents-8c20ddfb">Barrons</a>, <a href="https://x.com/karpathy/status/1882544526033924438">Karpathy</a>, <a href="https://openai.com/index/introducing-operator/">OpenAI</a></p>
        
        ---
        class: center, middle
        # Agents in 3 Levels of Complexity

        ---

        # Level 1: LLM + functions inside the prompt

        --
        
        - Inspired by ['Toolformer'](https://arxiv.org/pdf/2302.04761.pdf)

        <img style="width: 400px "src="../notebooks/assets-resources/toolformer.png">

        <p style="font-size: 14px; margin-left: 0px">
          <a href='https://arxiv.org/pdf/2302.04761.pdf'> Schick et al. 2023</a>
        </p>

        ---

        # Level 1: LLM + functions inside the prompt 

        

        <div style="display: flex; justify-content: center;">
          <img style="width: 100%" src="../notebooks/assets-resources/level1-agents.png" alt="LLM Level 1 Agents">
        </div>
        
        ---
        # Example from Toolformer Paper

        ```
        Wikipedia Search We use the following prompt
        for the Wikipedia search tool:
        Your task is to complete a given piece
        of text. You can use a Wikipedia Search
        API to look up information. You can do
        so by writing "[WikiSearch(term)]" where
        "term" is the search term you want to
        look up. Here are some examples of API
        calls:
        ```
        ---
        # Example from Toolformer Paper

        ```
        Wikipedia Search We use the following prompt
        for the Wikipedia search tool:
        Your task is to complete a given piece
        of text. You can use a Wikipedia Search
        API to look up information. You can do
        so by writing "[WikiSearch(term)]" where
        "term" is the search term you want to
        look up. Here are some examples of API
        calls:
        Input: The colors on the flag of Ghana
        have the following meanings: red is for
        the blood of martyrs, green for forests,
        and gold for mineral wealth.
        Output: The colors on the flag of Ghana
        have the following meanings: red is for
        [WikiSearch("Ghana flag red meaning")]
        the blood of martyrs, green for forests,
        and gold for mineral wealth.
        ....(more input output examples)...
        Input: x
        Output:
        ```

        <p style="font-size: 14px; margin-left: 0px">
          <a href='https://arxiv.org/pdf/2302.04761.pdf'> Schick et al. 2023</a>
        </p>
        ---

        

        # Limitations

        --

        - __Probabilistic outputs__ make function calls unreliable
        
        --

        - Need for __structured ways to prepare the inputs__ of 
        the function calls
        
        --
        
        - Putting entire functions inside text prompts is clunky and
        __non-scalable__
        
        --

        - Solution? Function Calling!
        
        ---
        class: center, middle

        # Level 2: OpenAI Function Calling

        ---

        # Level 2: OpenAI Function Calling

        <img src="../notebooks/assets-resources/level2-agents.png" style="width: 68%;">

        ---

        # Level 2: OpenAI Function Calling

        1. Call model with functions defined – along with your system and user messages.

        --

        2. Model decides to call function(s) – model returns the name and input arguments.

        --

        3. Execute function code – parse the model's response and handle function calls.

        --

        4. Supply model with results – so it can incorporate them into its final response.

        <p style="font-size: 14px; margin-top: 10px;"></p>
          <a href="https://platform.openai.com/docs/guides/function-calling">OpenAI Function Calling Docs</a>
        </p>


        ---        

        ???
        - __Takeaway__: This streamlined interaction sets the stage for the more advanced capabilities provided by the AutoGen framework, focusing on creating efficient, self-improving agents that can communicate with each other.

        ---
        class: center, middle

        # Level 3: Autonomous Agents

        ---
        # Level 3: Autonomous Agents

        
  
        <img style="width: 120%; margin-left: -75px" src="../notebooks/assets-resources/agent-loop.png">

        <p style="font-size: 14px; margin-top: 120px; margin-left: 0px;">
           <a href="https://blog.langchain.dev/openais-bet-on-a-cognitive-architecture/#:~:text=sweep.dev%20is%20another%20great%20example.%20they%20wrote%20a%20blog%20over%20the%20summer%20describing%20their%20cognitive%20architecture%2C%20including%20a%20fantastic%20diagram.">Inspired by this article: 'OpenAI's Bet on a Cognitive Architecture'</a>
        </p>
                
        ---
        class: center, middle

        <h2>
        <span style="background-color: lightgreen">
         Notebook Demo: Building a Simple React Agent with LangChain/LangGraph
        </span>
        </h2>
        
        ---
        class: center, middle
        
        # Q&A

        ---
        class: center, middle

        # Break 

        ---
        class: center, middle

        # The Agent Loop

        ---
        # How Can We Effectively Perform Tasks with Agents?

        --
  
        <img style="width: 120%; margin-left: -75px" src="../notebooks/assets-resources/agent-loop.png">

        <p style="font-size: 14px; margin-top: 120px; margin-left: 0px;">
          <a href="https://blog.langchain.dev/openais-bet-on-a-cognitive-architecture/#:~:text=sweep.dev%20is%20another%20great%20example.%20they%20wrote%20a%20blog%20over%20the%20summer%20describing%20their%20cognitive%20architecture%2C%20including%20a%20fantastic%20diagram.">Inspired by this article: 'OpenAI's Bet on a Cognitive Architecture'</a>
        </p>
        ---
        # Good Agents are Routers

        --

        - Good examples of 'useful' agents (that implement a more like routing type of architecture than actual agent architecture)

        --

        - LangChain is a framework to implement these types of routing procedures!

        <p style="font-size: 14px; margin-top: 380px; margin-left: 20px;">
          <a href="https://blog.langchain.dev/openais-bet-on-a-cognitive-architecture/#:~:text=sweep.dev%20is%20another%20great%20example.%20they%20wrote%20a%20blog%20over%20the%20summer%20describing%20their%20cognitive%20architecture%2C%20including%20a%20fantastic%20diagram.">Inspired by this article: 'OpenAI's Bet on a Cognitive Architecture'</a>
        </p>
        ---

        # What is LangChain?

        --

        - [LangChain](https://python.langchain.com/docs/get_started/introduction) is a framework for building
        context-aware reasoning applications

        --

        - Its main features

        --

          - __Components__: composable tools and integrations 
          for working with language models. 
        --
        

          - __Off-the-shelf chains__: built-in assemblages of components for 
          accomplishing higher-level tasks

          <p style="font-size: 14px; margin-top: 250px;">
            <a href="https://python.langchain.com/docs/get_started/introduction" >LangChain Docs</a>
        

        ---
        # The LangChain Stack

        <img src="../notebooks/assets-resources/langchain_stack_112024_dark.svg" style="width: 90%;">

        <p style="font-size: 14px;">
          <a href="https://python.langchain.com/docs/get_started/introduction" >LangChain Docs</a>
        </p>

        ---

        # Core Elements of LangChain

        --

        ## __Models__

        --

        - Abstractions over LLM APIs (e.g ChatGPT API)

        --

        ```python
        from langchain_openai.chat_models import ChatOpenAI

        chat_model = ChatOpenAI(api_key=os.getenv("OPENAI_API_KEY"), model="gpt-4o")

        chat_model.invoke("hi!")
        ```

        --

        OR

        ```python
        # More flexible way to initialize the model (with any provider)
        from langchain.chat_models import init_chat_model

        model = init_chat_model("gpt-4o-mini", model_provider="openai") 
        ```
        
        ---

        # Core Elements of LangChain
        
        ## __Prompt Templates__

        --

        - Abstractions over traditional text prompts for LLMs

        --

        ```python
        from langchain.prompts import ChatPromptTemplate
        prompt = ChatPromptTemplate.from_template(template="Name 5 ideas for this product: {product}.")
        prompt.format(product="dog")
        # Output
        # 'Name 5 ideas for this product: dog'
        ```
        
        ---

        # Core Elements of LangChain

        ## __Output parser__
        
        --

        - Abstractions for parsing outputs of LLMs 

        --
        
        ```python
        model = ChatOpenAI(temperature=0)
        # Define your desired data structure.
        class Joke(BaseModel):
            setup: str = Field(description="question to set up a joke")
            punchline: str = Field(description="answer to resolve the joke")
        
            # Set up a parser + inject instructions into the prompt template.
        parser = JsonOutputParser(pydantic_object=Joke)
        
        prompt = PromptTemplate(
            template="Answer the user query.\n{format_instructions}\n{query}\n",
            input_variables=["query"],
            partial_variables={"format_instructions": parser.get_format_instructions()},
        )
        # LCEL chain interface
        chain = prompt | model | parser
        
        chain.invoke({"query": "Tell me a joke."})
        # Output
        # {'setup': 'Why did the chicken cross the road?', 
        #'punchline': 'To get to the other side!'}
        ```

        ---

        # LCEL - Putting Components Together

        --

        ## LCEL interface 

        - Interface that leverages the __`|`__ pipe symbol 
        to compose LangChain components

        --
        
        ```python
        from langchain_openai.chat_models import ChatOpenAI
        from langchain.prompts import ChatPromptTemplate
        from langchain.schema.output_parser import StrOutputParser


        model = ChatOpenAI(temperature=0)
        prompt = ChatPromptTemplate.from_template(template="Name 5 ideas for this product: {product}.")
        output_parser = StrOutputParser()
        
        chain = prompt | model | output_parser

        chain.invoke({"concept": "probability distribution"})
        # Output
        # - Discrete probability distribution: This concept... 
        # - Continuous probability.... 
        # ...
        ```

        ---
        class: center, middle

        <h1>
        <span style="background-color: lightgreen">
         Whiteboard Demo: Core Components of LangChain
        </span>
        </h1>

        ---
        class: center, middle

        <h1>
          <span style="background-color: lightgreen">
           Notebook Demo: Building a Simple LangChain App
          </span>
        </h1>



        ---
        class: center, middle

        # Break

        ---

        ## Advantages of LLM Agents

        --
        
        - **Flexibility**: Agents can adapt to various tasks by determining the best action to take.

        --
        
        - **Specialization**: LLM agents can be specialized with a set of tools to perform niche tasks.

        --
        
        - **Multi-Agent Collaboration**: Specialized LLM agents can collaborate to perform complex tasks.

        <p class="footnote">
            <a href='https://blog.langchain.dev/langgraph-multi-agent-workflows/'> LangGraph: Multi-Agent Workflows </a>
        </p>

        ---
        
        # Building Agents with LangChain

        --

        <span style="color: orange">-  LangChain gives us the material for the components</span>

        --

        <img src="../notebooks/assets-resources/components-langchain.png" width="800">

        ---
        class: center, middle

        # But that is half the puzzle!
        We still need fine-grained control over the connections between components and LCEL is not enough!

        ---
        # Understanding Agentic Systems
        
        - **Agentic Systems**: Systems that utilize large language models (LLMs) to manage the control flow of applications.
        
        --
        
        - **Key Functions**: Routing decisions, tool selection, and evaluating output sufficiency.

        --
        
        - **Agent Loop**: Continuous decision-making process that enables agents to solve complex tasks.

        <p class="footnote">
            <a href='https://langchain-ai.github.io/langgraph/concepts/high_level/#deployment'> Understanding agentic systems in LangGraph. </a>
        </p>

        ---
        ## Key Components of Agentic Systems

        --
        
        1. **Tool Calling**: Utilizing external tools to perform tasks.

        --

        2. **Action Taking**: Executing actions based on LLM outputs.

        --

        3. **Memory**: Keeping track of interactions for context-aware responses.

        --
        
        4. **Planning**: Structuring steps to ensure optimal decision-making.

        <p class="footnote">
            <a href='https://langchain-ai.github.io/langgraph/concepts/high_level/#deployment'> Key components of agentic systems. </a>
        </p>

        ---
        class: center, middle
        
        # Solution = LangGraph

        ---
        # LangGraph = LangChain + Graph Building Capabilities
        
        -  LangGraph gives us <span style="color: orange">components from LangChain</span> <span style="color: blue">flexible control for building graphs</span>

        <img src="../notebooks/assets-resources/langgraph-graph.png" width="800">


        ---
        # LangGraph = Deterministic + Non-Deterministic Control Flows

        - With that we get the ability to build workflows that mix probabilistic outputs (LLMs) + deterministic control flows (Python code)
        

        ---

        # Agents as Graphs

        --

        - Workflows built with agents are usually structured as graphs!!

        <img src="../notebooks/assets-resources/basic-agent-loop-abstractions.png" width="100%">

        ---

        # Agent Loop in LangGraph

        - Outline of an basic agent loop in langgraph:

        --

        <img src="../notebooks/assets-resources/basic-agent-diagram-langgraph.png" width="100%">

        ---

        
        # Why LangGraph?
        
        LangGraph is designed for building agentic applications with some core principles:

        --
        
        - **Controllability**: Offers low-level control which increases reliability in agentic systems.

        --
        
        - **Human-in-the-Loop**: Built-in persistence layer enhances human-agent interaction patterns.

        --
        
        - **Streaming First**: Supports streaming of events and tokens, providing real-time feedback to users.
        
        <p class="footnote">
            <a href='https://langchain-ai.github.io/langgraph/concepts/high_level/#why-langgraph'> Overview of LangGraph's purpose and principles. </a>
        </p>
        
        ---
        class: center, middle

        <h2>
        <span style="background-color: lightgreen">
         Notebook Demo: Building a Basic LangGraph Agent
        </span>
        </h2>

        ---
        class: center, middle

        # Q&A & Break

        ---
        
        ## The Basic Components of LangGraph
        
        LangGraph models agent workflows as graphs:

        --

        <img src="../notebooks/assets-resources/langgraph-components.png" style="width:400px; margin-left: 150px;">

        --
        
        - **Nodes**: Python functions that implement the logic of agents, taking the current State as input and returning an updated State.

        --
        
        - **Edges/Conditional Edges**: Functions that implement fixed/conditional transitions to determine which Node to execute next based on the current State.

        <p class="footnote">
          <a href='https://langchain-ai.github.io/langgraph/concepts/low_level/#update-state'> Explanation of LangGraph's components. </a>
        </p>
        
        ---
        
        ## States in LangGraph
        
        These graphs in LangGraph are driven by:

        --
        
        - **States**: Shared data structures that evolve over time as Nodes execute and pass messages along Edges.

        --
        
        - **Message Passing**: Nodes send messages to activate other Nodes, facilitating the execution of workflows in discrete iterations or "super-steps".
        
        <p class="footnote">
            <a href='https://langchain-ai.github.io/langgraph/concepts/low_level/#update-state'> Overview of how graphs and states interact in LangGraph. </a>
        </p>

        ---
        
        ## Nodes
        In LangGraph, Nodes are the core functional units:

        --

        - **Functionality**: Each Node is a Python function that processes the current State and outputs an updated State.

        --
        
        - **Execution**: Nodes can run synchronously or asynchronously, and are added to the graph using the `add_node` method.

        --
        
        - **Special Nodes**: Includes START and END Nodes to manage the flow of execution in the graph.
        
        <p class="footnote">
            <a href='https://langchain-ai.github.io/langgraph/concepts/low_level/#update-state'> Details on the functionality and structure of Nodes in LangGraph. </a>
        </p>
        

        ---
        
        ## Edges
        
        Edges define the routing logic in LangGraph:
        
        - **Types of Edges**:

        --
          
          - **Normal Edges**: Direct transitions from one Node to another.
        
        --
          
          - **Conditional Edges**: Determine the next Node(s) to execute based on a function's output.
        
        --
          
          - **Entry Points**: Specify which Node to invoke first based on user input.
        
        --
        
        - **Parallel Execution**: Multiple outgoing edges from a Node can trigger parallel execution of destination Nodes.
        
        <p class="footnote">
            <a href='https://langchain-ai.github.io/langgraph/concepts/low_level/#update-state'> Explanation of Edges and their role in LangGraph. </a>
        </p>
        
        ---
        class: center, middle

        <h1>
        <span style="background-color: lightgreen">
         Notebook Demo: Building a Local LangGraph Agent 
        </span>
        </h1>

        ---
        class: center, middle

        # Q&A & Break


        ---

        # Practical Use Case: Customer Support Agent

        --
        
        - **Scenario**: An LLM-powered customer support agent.

        --
        
        - **User Input**: Customer asks about order status.

        --
        
        - **LLM Decision**: Determines if it can provide the status directly or if it needs to fetch data from the database.

        --
        
        - **Action Taken**: If data fetch is needed, the agent queries the database and updates the user with the order status.

        <p style="font-size: 14px; margin-top: 200px; margin-left: 0px">
          <a href='https://langchain-ai.github.io/langgraph/concepts/high_level/#deployment'> Practical use case of LLM agents in customer support. </a>
        </p>

        ---
        class: center, middle

        <h1>
          <span style="background-color: lightgreen">
           Notebook Demo: Building a Research Agent in LangGraph 
          </span>
        </h1>

        ---
        class: center, middle

        <h1>
          <span style="background-color: lightgreen">
           Whiteboard & Notebook Demo: Agents in 2025 - LangChain, LangGraph and MCPs
          </span>
        </h1>

        ---


        # References

          1. [Toolformer - Schick et al., 2023](https://arxiv.org/pdf/2302.04761.pdf)
          2. [ReACT - Yao, X., et al., 2023](https://arxiv.org/pdf/2210.03629.pdf)
          3. [A Survey on Large Language Model based Autonomous Agents - Wang et al., 2023](https://arxiv.org/pdf/2308.11432.pdf)
          4. [BabyAGI](https://github.com/yoheinakajima/babyagi)
          5. [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
          6. [GPT-Researcher](https://github.com/assafelovic/gpt-researcher?ref=blog.langchain.dev)
          7. [Custom GPTs](https://openai.com/blog/introducing-gpts)
          8. [OpenAI function calling Docs](https://platform.openai.com/docs/guides/function-calling)
          9. [OpenAI's Bet on a Cognitive Architecture](https://blog.langchain.dev/openais-bet-on-a-cognitive-architecture/#:~:text=sweep.dev%20is%20another%20great%20example.%20they%20wrote%20a%20blog%20over%20the%20summer%20describing%20their%20cognitive%20architecture%2C%20including%20a%20fantastic%20diagram.)
          10. [LangChain Docs](https://python.langchain.com/docs/get_started/introduction)

        ---

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>