{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inspired by this Anthropic's blog post:\n",
    "- https://www.anthropic.com/research/building-effective-agents\n",
    "\n",
    "and this source code:\n",
    "- https://github.com/anthropics/anthropic-cookbook/blob/main/patterns/agents/basic_workflows.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will implement the essential patterns for working with LLMs that serve as a great introduction to LangChain components!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'oreilly-agents (Python 3.11.11)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. WebSocket is not defined"
     ]
    }
   ],
   "source": [
    "# %pip install openai>1.50.0 langchain>0.3.0 langgraph langchainhub langchain-openai langchain-community langchain-cli langchain_ollama tavily-python>=0.5.0 langchain_nomic nomic[local] langserve faiss-cpu tiktoken pypdf chroma jira google-search-results numexpr beautifulsoup4 scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def chain(input, prompts):\n",
    "    result = input\n",
    "    for i, prompt in prompts:\n",
    "        print(f\"Step: {i}\")\n",
    "        result = llm_call(f\"{prompt}: \\n\\n{result}\")\n",
    "        print(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis chain...\n",
      "Step: 1\n"
     ]
    }
   ],
   "source": [
    "# Example prompts for a multi-step analysis chain\n",
    "analysis_prompts = [\n",
    "    (1, \"Analyze this text and identify the main themes and topics\"),\n",
    "    (2, \"For each theme identified, provide specific examples from the text\"),\n",
    "    (3, \"Synthesize the themes and examples into a coherent summary\"),\n",
    "    (4, \"Generate 3 discussion questions based on the analysis\")\n",
    "]\n",
    "\n",
    "# Sample input text\n",
    "sample_text = \"\"\"\n",
    "The rise of artificial intelligence has transformed multiple industries in the past decade. \n",
    "Healthcare has seen improvements in diagnosis accuracy and treatment planning. \n",
    "Manufacturing has become more efficient with predictive maintenance and automated quality control.\n",
    "However, these advancements also raise important ethical questions about privacy, job displacement, \n",
    "and the role of human decision-making in an increasingly automated world.\n",
    "\"\"\"\n",
    "\n",
    "# Run the chain\n",
    "print(\"Running analysis chain...\")\n",
    "final_result = chain(sample_text, analysis_prompts)\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel(prompt, inputs, n_workers=3):\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = [executor.submit(llm_call, f\"{prompt}: \\n {input}\") for input in inputs]\n",
    "    \n",
    "    return [f.result() for f in futures]\n",
    "\n",
    "\n",
    "prompt = \"Summarize this paragraph in one sentence\"\n",
    "inputs = [\n",
    "    \"\"\"\n",
    "    Climate change is causing rising sea levels and extreme weather patterns globally. \n",
    "    Scientists have observed accelerating ice melt in polar regions, leading to coastal \n",
    "    flooding in low-lying areas. Additionally, communities worldwide are experiencing \n",
    "    more frequent and severe hurricanes, droughts, and other extreme weather events. \n",
    "    These changes pose significant risks to agriculture, infrastructure, and human \n",
    "    populations, particularly in vulnerable coastal regions and developing nations.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    The development of quantum computers represents a major technological breakthrough\n",
    "    with far-reaching implications. These powerful machines leverage quantum mechanical\n",
    "    properties to perform complex calculations exponentially faster than classical computers.\n",
    "    In the field of cryptography, quantum computers could break many current encryption\n",
    "    methods while enabling new unbreakable encryption protocols. For drug discovery,\n",
    "    quantum computers could simulate molecular interactions with unprecedented accuracy,\n",
    "    potentially accelerating the development of new medicines and treatments for diseases.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Social media platforms have transformed how people communicate and share information\n",
    "    in the modern digital age. Platforms like Facebook, Twitter, and Instagram have created\n",
    "    virtual communities where users can instantly connect with friends and family across the\n",
    "    globe. These platforms enable the rapid spread of news, ideas, and cultural trends,\n",
    "    while also raising concerns about privacy, misinformation, and the impact on mental\n",
    "    health. The rise of social media has also revolutionized marketing, activism, and\n",
    "    how businesses engage with their customers.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "parallel(prompt, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Router Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field \n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ProfileRouter(BaseModel):\n",
    "    selected_profile: str = Field(description=\"You return one of 3 profiles: [hr, software engineer, product manager]\")\n",
    "    \n",
    "\n",
    "def llm_call_route(problem):\n",
    "    '''Selects which profile to use to analyse the input.'''\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'system', 'content': \"\"\"\n",
    "                   You select which of the following profiles should be used to solve a problem given as input:\n",
    "                   1. hr\n",
    "                   2. software engineer\n",
    "                   3. product manager\n",
    "                   Your output is solely the choosen profile string.\n",
    "                   \"\"\"},\n",
    "                  {'role': 'user', 'content': f\"We had this problem: \\n {problem}. Output the appropriate profile to solve it:\\n\"}],\n",
    "        response_format=ProfileRouter\n",
    "    )\n",
    "    return response.choices[0].message.parsed.selected_profile\n",
    "    \n",
    "\n",
    "llm_call_route(\"I need to fix this bug in our codebase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def router(input, routes: Dict[str, str]):\n",
    "    \n",
    "    route_response = llm_call_route(input)\n",
    "    \n",
    "    selected_prompt = routes[route_response]\n",
    "    \n",
    "    return llm_call(selected_prompt + f\"Input: {input}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_profiles = {\n",
    "    'hr': \"\"\"You are an experienced HR professional focused on employee relations, recruitment, and workplace policies.\n",
    "    Your expertise includes conflict resolution, talent acquisition, performance management, and ensuring compliance with labor laws.\n",
    "    Analyze the input from an HR perspective and provide appropriate guidance.\"\"\",\n",
    "    \n",
    "    'software engineer': \"\"\"You are a senior software engineer with deep technical expertise in software development, architecture, and best practices.\n",
    "    Your skills include debugging, code optimization, system design, and technical problem-solving.\n",
    "    Analyze the input from an engineering perspective and provide technical solutions.\"\"\",\n",
    "    \n",
    "    'product manager': \"\"\"You are a seasoned product manager skilled in product strategy, user experience, and market analysis.\n",
    "    Your expertise includes feature prioritization, roadmap planning, stakeholder management, and data-driven decision making.\n",
    "    Analyze the input from a product perspective and provide strategic recommendations.\"\"\"\n",
    "}\n",
    "\n",
    "problems = [\n",
    "    \"We need to improve our onboarding process for new employees to increase retention\",\n",
    "    \"The authentication service is throwing intermittent 500 errors in production\",\n",
    "    \"Our user engagement metrics have dropped 20% since the last release\"\n",
    "]\n",
    "\n",
    "# Test the router with each problem\n",
    "for problem in problems:\n",
    "    print(f\"\\nProblem: {problem}\")\n",
    "    print(f\"Response: {router(problem, routes_profiles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that've learned these 3 basic patterns let's look at them implemented with LangChain to understand what LangChain tries to introduce with it's framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, first thing to note before reproducing the chain pattern in langchain is nomenclature, in LangChain, they define chain\n",
    "as a composable building block built on top of a specialized interface named: [`Runnable Interface`](https://python.langchain.com/docs/concepts/runnables/), so instead of it being this action of chaining things together, it's the chainable thing itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the concept remains the same:\n",
    "\n",
    "- Composing calls to an LLM\n",
    "\n",
    "Let's see how that would be done with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we wrote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def chain(input, prompts):\n",
    "    result = input\n",
    "    for i, prompt in prompts:\n",
    "        print(f\"Step: {i}\")\n",
    "        result = llm_call(f\"{prompt}: \\n\\n{result}\")\n",
    "        print(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `llm_call()` function calls the LLM simply.\n",
    "- The `chain` function takes in an input and a list of prompts and runs all the prompts on that input in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- This approach means that if we wanted each prompt for example to be used with a different LLM, that would require rewriting the implementations.\n",
    "\n",
    "LangChain assumes this flexibility from the start! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] =\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" \n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"default\"  # if not specified, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def llm_call(prompt):\n",
    "    return ChatOpenAI(model=\"gpt-4o-mini\").invoke(prompt).content\n",
    "\n",
    "llm_call(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_prompts = [\n",
    "    (1, \"Analyze this text and identify the main themes and topics\"),\n",
    "    (2, \"For each theme identified, provide specific examples from the text\"),\n",
    "    (3, \"Synthesize the themes and examples into a coherent summary\"),\n",
    "    (4, \"Generate 3 discussion questions based on the analysis\")\n",
    "]\n",
    "\n",
    "# Sample input text\n",
    "sample_text = \"\"\"\n",
    "The rise of artificial intelligence has transformed multiple industries in the past decade. \n",
    "Healthcare has seen improvements in diagnosis accuracy and treatment planning. \n",
    "Manufacturing has become more efficient with predictive maintenance and automated quality control.\n",
    "However, these advancements also raise important ethical questions about privacy, job displacement, \n",
    "and the role of human decision-making in an increasingly automated world.\n",
    "\"\"\"\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Analyze this text and identify the main themes and topics: {input}\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "chain1 = prompt1 | llm\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\"For each theme identified, provide specific examples from the text: {input}\")\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "prompt3 = ChatPromptTemplate.from_template(\"Synthesize the themes and examples into a coherent summary: {input}\")\n",
    "chain3 = prompt3 | llm\n",
    "\n",
    "prompt4 = ChatPromptTemplate.from_template(\"Generate 3 discussion questions based on the analysis: {input}\")\n",
    "chain4 = prompt4 | llm\n",
    "\n",
    "final_chain = chain1 | chain2 | chain3 | chain4\n",
    "\n",
    "output = final_chain.invoke({\"input\": sample_text})\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could simplify this by writing a function to abstract the creation of each chain block (runnable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "def create_runnable_chain(prompt):\n",
    "    return ChatPromptTemplate.from_template(prompt) | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of create_runnable_chain\n",
    "prompts = [\n",
    "    \"Analyze this text and identify the main themes and topics: {input}\",\n",
    "    \"For each theme identified, provide specific examples from the text: {input}\",\n",
    "    \"Synthesize the themes and examples into a coherent summary: {input}\",\n",
    "    \"Generate 3 discussion questions based on the analysis: {input}\"\n",
    "]\n",
    "\n",
    "# Create individual chains\n",
    "chains = [create_runnable_chain(prompt) for prompt in prompts]\n",
    "\n",
    "# Combine chains into a sequence\n",
    "# Below is equivalent to: final_chain = chain1 | chain2 | ...\n",
    "final_chain = RunnableSequence(*chains)\n",
    "\n",
    "# Sample input text\n",
    "sample_text = \"\"\"\n",
    "The rise of artificial intelligence has transformed multiple industries in the past decade.\n",
    "Healthcare has seen improvements in diagnosis accuracy and treatment planning.\n",
    "Manufacturing has become more efficient with predictive maintenance and automated quality control.\n",
    "However, these advancements also raise important ethical questions about privacy, job displacement,\n",
    "and the role of human decision-making in an increasingly automated world.\n",
    "\"\"\"\n",
    "\n",
    "# Run the chain\n",
    "output = final_chain.invoke({\"input\": sample_text})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option would be to use the convenient `@chain` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def custom_chain(text):\n",
    "    # Analyze themes\n",
    "    analyze_chain = ChatPromptTemplate.from_template(\"Analyze this text and identify the main themes and topics: {input}\") | llm\n",
    "    themes = analyze_chain.invoke({\"input\": text})\n",
    "    \n",
    "    # Get examples\n",
    "    examples_chain = ChatPromptTemplate.from_template(\"For each theme identified, provide specific examples from the text: {input}\") | llm\n",
    "    examples = examples_chain.invoke({\"input\": themes.content})\n",
    "    \n",
    "    # Synthesize\n",
    "    synthesis_chain = ChatPromptTemplate.from_template(\"Synthesize the themes and examples into a coherent summary: {input}\") | llm\n",
    "    synthesis = synthesis_chain.invoke({\"input\": examples.content})\n",
    "    \n",
    "    # Generate questions\n",
    "    questions_chain = ChatPromptTemplate.from_template(\"Generate 3 discussion questions based on the analysis: {input}\") | llm\n",
    "    return questions_chain.invoke({\"input\": synthesis.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "The rise of artificial intelligence has transformed multiple industries in the past decade.\n",
    "Healthcare has seen improvements in diagnosis accuracy and treatment planning.\n",
    "Manufacturing has become more efficient with predictive maintenance and automated quality control.\n",
    "However, these advancements also raise important ethical questions about privacy, job displacement,\n",
    "and the role of human decision-making in an increasingly automated world.\n",
    "\"\"\"\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "custom_chain.invoke(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our previous implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel(prompt, inputs, n_workers=3):\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = [executor.submit(llm_call, f\"{prompt}: \\n {input}\") for input in inputs]\n",
    "    \n",
    "    return [f.result() for f in futures]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with langchain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "prompt_template = \"Summarize this paragraph in one sentence: {input}\"\n",
    "inputs = [\n",
    "    \"\"\"\n",
    "    Climate change is causing rising sea levels and extreme weather patterns globally. \n",
    "    Scientists have observed accelerating ice melt in polar regions, leading to coastal \n",
    "    flooding in low-lying areas. Additionally, communities worldwide are experiencing \n",
    "    more frequent and severe hurricanes, droughts, and other extreme weather events. \n",
    "    These changes pose significant risks to agriculture, infrastructure, and human \n",
    "    populations, particularly in vulnerable coastal regions and developing nations.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    The development of quantum computers represents a major technological breakthrough\n",
    "    with far-reaching implications. These powerful machines leverage quantum mechanical\n",
    "    properties to perform complex calculations exponentially faster than classical computers.\n",
    "    In the field of cryptography, quantum computers could break many current encryption\n",
    "    methods while enabling new unbreakable encryption protocols. For drug discovery,\n",
    "    quantum computers could simulate molecular interactions with unprecedented accuracy,\n",
    "    potentially accelerating the development of new medicines and treatments for diseases.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Social media platforms have transformed how people communicate and share information\n",
    "    in the modern digital age. Platforms like Facebook, Twitter, and Instagram have created\n",
    "    virtual communities where users can instantly connect with friends and family across the\n",
    "    globe. These platforms enable the rapid spread of news, ideas, and cultural trends,\n",
    "    while also raising concerns about privacy, misinformation, and the impact on mental\n",
    "    health. The rise of social media has also revolutionized marketing, activism, and\n",
    "    how businesses engage with their customers.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = prompt | llm \n",
    "\n",
    "chain.batch([{\"input\":input} for input in inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Router Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define classification chain\n",
    "classification_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user problem below, classify it as either `HR`, `Software Engineer`, or `Product Manager`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<problem>\n",
    "{problem}\n",
    "</problem>\n",
    "\n",
    "Classification:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Define sub-chains\n",
    "hr_chain = PromptTemplate.from_template(\n",
    "    \"\"\"You are an experienced HR professional focused on employee relations, recruitment, and workplace policies.\n",
    "Your expertise includes conflict resolution, talent acquisition, performance management, and ensuring compliance with labor laws.\n",
    "Analyze the input from an HR perspective and provide appropriate guidance.\n",
    "\n",
    "Problem: {problem}\n",
    "Solution:\"\"\"\n",
    ") | ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "software_engineer_chain = PromptTemplate.from_template(\n",
    "    \"\"\"You are a senior software engineer with deep technical expertise in software development, architecture, and best practices.\n",
    "Your skills include debugging, code optimization, system design, and technical problem-solving.\n",
    "Analyze the input from an engineering perspective and provide technical solutions.\n",
    "\n",
    "Problem: {problem}\n",
    "Solution:\"\"\"\n",
    ") | ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "product_manager_chain = PromptTemplate.from_template(\n",
    "    \"\"\"You are a seasoned product manager skilled in product strategy, user experience, and market analysis.\n",
    "Your expertise includes feature prioritization, roadmap planning, stakeholder management, and data-driven decision-making.\n",
    "Analyze the input from a product perspective and provide strategic recommendations.\n",
    "\n",
    "Problem: {problem}\n",
    "Solution:\"\"\"\n",
    ") | ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# Define routing function\n",
    "def route(info):\n",
    "    if \"hr\" in info[\"category\"].lower():\n",
    "        print(\"Classified as HR.\")\n",
    "        return hr_chain\n",
    "    elif \"software engineer\" in info[\"category\"].lower():\n",
    "        print(\"Classified as Software Engineer.\")\n",
    "        return software_engineer_chain\n",
    "    elif \"product manager\" in info[\"category\"].lower():\n",
    "        print(\"Classified as Product Manager.\")\n",
    "        return product_manager_chain\n",
    "    else:\n",
    "        return product_manager_chain  # Default fallback\n",
    "\n",
    "# Create runnable pipeline\n",
    "full_chain = {\"category\": classification_chain, \"problem\": lambda x: x[\"problem\"]} | RunnableLambda(route)\n",
    "\n",
    "# Example problems\n",
    "problems = [\n",
    "    \"We need to improve our onboarding process for new employees to increase retention\",\n",
    "    \"The authentication service is throwing intermittent 500 errors in production\",\n",
    "    \"Our user engagement metrics have dropped 20% since the last release\"\n",
    "]\n",
    "\n",
    "# Invoke the chain for each problem\n",
    "for problem in problems:\n",
    "    response = full_chain.invoke({\"problem\": problem})\n",
    "    print(f\"Problem: {problem}\\nSolution: {response}\\n{'-' * 50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only new concept here is that we used:\n",
    "[`RunnableLambda`](https://arc.net/l/quote/ovwpozka)\n",
    "to setup our custom logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
