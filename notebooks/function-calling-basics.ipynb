{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c33cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd6dd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final input:\n",
      "[{'role': 'user', 'content': 'What is my horoscope? I am an Aquarius.'}, ResponseReasoningItem(id='rs_03619f506fad52f600698b5213e15481949fe2b7f5c630572f', summary=[], type='reasoning', encrypted_content=None, status=None), ResponseFunctionToolCall(arguments='{\"sign\":\"Aquarius\"}', call_id='call_IImhMYakfvKjDjJU9OeYXPv7', name='get_horoscope', type='function_call', id='fc_03619f506fad52f600698b52164220819486f17f29065128c5', status='completed'), {'type': 'function_call_output', 'call_id': 'call_IImhMYakfvKjDjJU9OeYXPv7', 'output': '{\"horoscope\": \"{\\'sign\\': \\'Aquarius\\'}: Next Tuesday you will befriend a baby otter.\"}'}]\n",
      "Final output:\n",
      "{\n",
      "  \"id\": \"resp_03619f506fad52f600698b5217eb0481948f91b0c423960a3a\",\n",
      "  \"created_at\": 1770738199.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": \"Respond only with a horoscope generated by a tool.\",\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-5-2025-08-07\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"msg_03619f506fad52f600698b52188b14819482dbe58b16d32733\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"{'sign': 'Aquarius'}: Next Tuesday you will befriend a baby otter.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": []\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"name\": \"get_horoscope\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"sign\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"An astrological sign like Taurus or Aquarius\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"sign\"\n",
      "        ],\n",
      "        \"additionalProperties\": false\n",
      "      },\n",
      "      \"strict\": true,\n",
      "      \"type\": \"function\",\n",
      "      \"description\": \"Get today's horoscope for an astrological sign.\"\n",
      "    }\n",
      "  ],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": null,\n",
      "  \"prompt\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": \"medium\",\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    },\n",
      "    \"verbosity\": \"medium\"\n",
      "  },\n",
      "  \"top_logprobs\": 0,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 247,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 23,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 270\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"billing\": {\n",
      "    \"payer\": \"developer\"\n",
      "  },\n",
      "  \"completed_at\": 1770738202,\n",
      "  \"frequency_penalty\": 0.0,\n",
      "  \"presence_penalty\": 0.0,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"prompt_cache_retention\": null,\n",
      "  \"safety_identifier\": null,\n",
      "  \"store\": true\n",
      "}\n",
      "\n",
      "{'sign': 'Aquarius'}: Next Tuesday you will befriend a baby otter.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define a list of callable tools for the model\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"get_horoscope\",\n",
    "        \"description\": \"Get today's horoscope for an astrological sign.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sign\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"An astrological sign like Taurus or Aquarius\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"sign\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Python function written outside of the context of the LLM but inside of our Python environment.\n",
    "def get_horoscope(sign):\n",
    "    return f\"{sign}: Next Tuesday you will befriend a baby otter.\"\n",
    "\n",
    "# Create a running input list we will add to over time\n",
    "input_list = [\n",
    "    {\"role\": \"user\", \"content\": \"What is my horoscope? I am an Aquarius.\"}\n",
    "]\n",
    "\n",
    "# 2. Prompt the model with tools defined\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    tools=tools, # this is where we inform the model about the tools we have available\n",
    "    input=input_list,\n",
    ")\n",
    "\n",
    "# Save function call outputs for subsequent requests\n",
    "input_list += response.output\n",
    "\n",
    "for item in response.output:\n",
    "    if item.type == \"function_call\":\n",
    "        if item.name == \"get_horoscope\":\n",
    "            # 3. Execute the function logic for get_horoscope\n",
    "            horoscope = get_horoscope(json.loads(item.arguments))\n",
    "            # we call this output: observation!\n",
    "            \n",
    "            # 4. Provide function call results to the model\n",
    "            input_list.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": item.call_id,\n",
    "                \"output\": json.dumps({\n",
    "                  \"horoscope\": horoscope\n",
    "                })\n",
    "            })\n",
    "\n",
    "print(\"Final input:\")\n",
    "print(input_list)\n",
    "\n",
    "# the second time we call the model\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    instructions=\"Respond only with a horoscope generated by a tool.\",\n",
    "    tools=tools,\n",
    "    input=input_list,\n",
    ")\n",
    "\n",
    "# 5. The model should be able to give a response!\n",
    "print(\"Final output:\")\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(\"\\n\" + response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31419e8c",
   "metadata": {},
   "source": [
    "# JS: What if the LLM does not have all of information it needs to call a function?  Will it ask for additional information (e.g., sign)\n",
    "\n",
    "YES usually it will but your code has to allow for the LLM Agent to get INPUT FROM THE USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa56675",
   "metadata": {},
   "source": [
    "# LR: How did the llm know to cazll get_horoscope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8075a031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather conditions in New York are partly cloudy with a temperature of 22°C.\n",
      "\n",
      "In London, it is rainy with a temperature of 15°C.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "def get_temperature(city: str) -> str:\n",
    "  \"\"\"Get the current temperature for a city\n",
    "  \n",
    "  Args:\n",
    "    city: The name of the city\n",
    "\n",
    "  Returns:\n",
    "    The current temperature for the city\n",
    "  \"\"\"\n",
    "  temperatures = {\n",
    "    \"New York\": \"22°C\",\n",
    "    \"London\": \"15°C\",\n",
    "    \"Tokyo\": \"18°C\"\n",
    "  }\n",
    "  return temperatures.get(city, \"Unknown\")\n",
    "\n",
    "def get_conditions(city: str) -> str:\n",
    "  \"\"\"Get the current weather conditions for a city\n",
    "  \n",
    "  Args:\n",
    "    city: The name of the city\n",
    "\n",
    "  Returns:\n",
    "    The current weather conditions for the city\n",
    "  \"\"\"\n",
    "  conditions = {\n",
    "    \"New York\": \"Partly cloudy\",\n",
    "    \"London\": \"Rainy\",\n",
    "    \"Tokyo\": \"Sunny\"\n",
    "  }\n",
    "  return conditions.get(city, \"Unknown\")\n",
    "\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'What are the current weather conditions and temperature in New York and London?'}]\n",
    "\n",
    "# The python client automatically parses functions as a tool schema so we can pass them directly\n",
    "# Schemas can be passed directly in the tools list as well \n",
    "# ollama is doing the automatic converstion of functions into tool schemas for the model to know which to sue\n",
    "response = chat(model='mistral-small3.2', messages=messages, tools=[get_temperature, get_conditions])\n",
    "\n",
    "# add the assistant message to the messages\n",
    "messages.append(response.message)\n",
    "if response.message.tool_calls:\n",
    "  # process each tool call \n",
    "  for call in response.message.tool_calls:\n",
    "    # execute the appropriate tool\n",
    "    if call.function.name == 'get_temperature':\n",
    "      result = get_temperature(**call.function.arguments)\n",
    "    elif call.function.name == 'get_conditions':\n",
    "      result = get_conditions(**call.function.arguments)\n",
    "    else:\n",
    "      result = 'Unknown tool'\n",
    "    # add the tool result to the messages\n",
    "    messages.append({'role': 'tool',  'tool_name': call.function.name, 'content': str(result)})\n",
    "\n",
    "  # generate the final response\n",
    "  final_response = chat(model='mistral-small3.2', messages=messages, tools=[get_temperature, get_conditions])\n",
    "  print(final_response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d0903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
