{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{}', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T13:13:19.108902Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1608720542, 'load_duration': 707797500, 'prompt_eval_count': 26, 'prompt_eval_duration': 868628208, 'eval_count': 2, 'eval_duration': 30773167, 'model_name': 'llama3.2'}, id='run-0a338f92-6d96-48e2-9fde-7cf0b8712c73-0', usage_metadata={'input_tokens': 26, 'output_tokens': 2, 'total_tokens': 28})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm_structured_output = ChatOllama(model=\"llama3.2\", format=\"json\", temperature=0)\n",
    "\n",
    "llm_structured_output.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Folder path was created at: lucas-test-agents'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_dir(folder_path):\n",
    "    \"\"\"\n",
    "    Creates a directory given a folder path.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "    \n",
    "    return f\"Folder path was created at: {folder_path}\"\n",
    "\n",
    "\n",
    "create_dir(\"lucas-test-agents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mlucas-test-agents/\u001b[m\u001b[m                \u001b[1m\u001b[36mpancakes-are-better-than-waffles/\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -d */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A file was created at: ./lucas-test-agents/file-text.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_file(file_path, contents=\"\"):\n",
    "    \"\"\"\n",
    "    Creates a file with content.\n",
    "    If no content is provided it will create an empty file.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(contents)\n",
    "    \n",
    "    return f\"A file was created at: {file_path}\"\n",
    "\n",
    "create_file(\"./lucas-test-agents/file-text.txt\", \"This is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads from file given its path.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        contents = f.read()\n",
    "    \n",
    "    return contents\n",
    "\n",
    "read_file(\"./lucas-test-agents/file-text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [create_dir, create_file, read_file]\n",
    "\n",
    "llm_tools = llm_structured_output.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt = f\"\"\"You are an agent that helps users with desktop tasks like reading and writing files and creating directories. You either call a tool\n",
    "from the options available: create_dir, create_file or read_file, or you return a summary of the tasks performed and at the end a string: END.\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "llm_agent = prompt_template | llm_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T13:14:14.480705Z', 'done': True, 'done_reason': 'stop', 'total_duration': 562376834, 'load_duration': 26569375, 'prompt_eval_count': 330, 'prompt_eval_duration': 252891542, 'eval_count': 30, 'eval_duration': 281725541, 'model_name': 'llama3.2'}, id='run-7080bd5a-082b-41cd-9639-5a26970a849d-0', tool_calls=[{'name': 'create_file', 'args': {'contents': '', 'file_path': 'lucas-locas-pancakes.md'}, 'id': 'e6ddc5cb-55ff-4ae1-8829-b97bb23b2a61', 'type': 'tool_call'}], usage_metadata={'input_tokens': 330, 'output_tokens': 30, 'total_tokens': 360})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_agent.invoke(\"Create file named lucas-locas-pancakes.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'create_file',\n",
       "  'args': {'contents': '', 'file_path': './test-agent.txt'},\n",
       "  'id': '85d84fe2-affb-43a1-98b1-ce56f7d58ac5',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tool_call = llm_agent.invoke(\"Create a file named ./test-agent.txt\")\n",
    "output_tool_call.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_mapping = {\n",
    "    \"create_file\": create_file,\n",
    "    \"create_dir\": create_dir,\n",
    "    \"read_file\": read_file\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(query, observations=[], actions_taken=[]):\n",
    "    \"\"\"\n",
    "    Calls the llm, it can return a tool call with arguments for calling different tools,\n",
    "    or an output to the user.\n",
    "    \"\"\"\n",
    "    template = \"\"\"\n",
    "    Imagine you are a simple assistant tasked with managing a file system. You have access to three tools:\n",
    "    \n",
    "    1. create_dir: Creates a new directory.\n",
    "    2. create_file: Creates a new file and optionally writes content to it.\n",
    "    3. read_file: Reads the contents of an existing file.\n",
    "\n",
    "    Based on the user input and your observations, choose an action to execute. Your action must follow these guidelines:\n",
    "    \n",
    "    * Action Guidelines *\n",
    "    1) Only one action is allowed per iteration.\n",
    "    2) Be concise and specific about what to create, write, or read.\n",
    "    3) Provide clear reasoning for your action.\n",
    "    4) Always consider the current context before taking action.\n",
    "    \n",
    "    Respond in the following format:\n",
    "    Thought: {{Your reasoning here}}\n",
    "    Action: {{Action name with parameters}}\n",
    "    \n",
    "    Available Actions:\n",
    "    - create_dir [directory_name]\n",
    "    - create_file [file_name]; [content (optional)]\n",
    "    - read_file [file_name]\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the input for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    {template}\n",
    "\n",
    "    User Query: {query}\n",
    "    Observations: {observations}\n",
    "    Actions Taken So Far: {actions_taken}\n",
    "    \"\"\"\n",
    "\n",
    "    output = llm_agent.invoke(prompt)\n",
    "\n",
    "    if output.tool_calls:\n",
    "        for tool_call in output.tool_calls:\n",
    "            function_name = tool_call[\"name\"]\n",
    "            function_args = tool_call.get(\"args\", [])  # default to empty list if \"args\" is missing\n",
    "\n",
    "            # Check if function_name exists in tool_mapping\n",
    "            if function_name not in tool_mapping:\n",
    "                print(f\"Error: Unknown function name '{function_name}'\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Attempt to call the function with the provided arguments\n",
    "                tool_output = tool_mapping[function_name](**function_args)\n",
    "                actions_taken.append(function_name)\n",
    "                observations.append(tool_output)\n",
    "            except TypeError as e:\n",
    "                # Handle errors if the number of arguments does not match the expected number for the function\n",
    "                print(f\"Error: Invalid arguments for function '{function_name}': {e}\")\n",
    "    \n",
    "    \n",
    "    return output, actions_taken, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T13:14:27.739121Z', 'done': True, 'done_reason': 'stop', 'total_duration': 489874833, 'load_duration': 33841916, 'prompt_eval_count': 554, 'prompt_eval_duration': 221545125, 'eval_count': 23, 'eval_duration': 233320834, 'model_name': 'llama3.2'}, id='run-29a1d5dc-8449-461b-8ad7-de495f781cc0-0', tool_calls=[{'name': 'create_dir', 'args': {'folder_path': 'lucas-tests-tool-calling'}, 'id': '0638836c-9d2f-4b34-ac00-b03ce31474c9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 554, 'output_tokens': 23, 'total_tokens': 577}),\n",
       " ['create_dir'],\n",
       " ['Folder path was created at: lucas-tests-tool-calling'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_call(\"Create a folder named: lucas-tests-tool-calling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T13:14:29.360564Z', 'done': True, 'done_reason': 'stop', 'total_duration': 595844708, 'load_duration': 35852167, 'prompt_eval_count': 587, 'prompt_eval_duration': 133772250, 'eval_count': 41, 'eval_duration': 424364083, 'model_name': 'llama3.2'}, id='run-27432488-cd86-4988-bf3c-c24abc15df9c-0', tool_calls=[{'name': 'create_file', 'args': {'contents': 'Hello Lucas! You just made a tool call!', 'file_path': './lucas-tests-tool-calling/test1.txt'}, 'id': 'cbfde7ff-9910-4f2d-8f0e-2bd4e85adea4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 587, 'output_tokens': 41, 'total_tokens': 628}),\n",
       " ['create_dir', 'create_file'],\n",
       " ['Folder path was created at: lucas-tests-tool-calling',\n",
       "  'A file was created at: ./lucas-tests-tool-calling/test1.txt'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_call(\"Create a file at: ./lucas-tests-tool-calling/test1.txt with the contents: Hello Lucas! You just made a tool call!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T13:14:30.642812Z', 'done': True, 'done_reason': 'stop', 'total_duration': 406097125, 'load_duration': 26081667, 'prompt_eval_count': 597, 'prompt_eval_duration': 79667583, 'eval_count': 26, 'eval_duration': 298713292, 'model_name': 'llama3.2'}, id='run-824ece0b-5acb-4d8b-a03f-bcbfe8734c7c-0', tool_calls=[{'name': 'read_file', 'args': {'file_path': './lucas-tests-tool-calling/test1.txt'}, 'id': '97109d3f-ca46-4243-b550-842257b9da56', 'type': 'tool_call'}], usage_metadata={'input_tokens': 597, 'output_tokens': 26, 'total_tokens': 623}),\n",
       " ['create_dir', 'create_file', 'read_file'],\n",
       " ['Folder path was created at: lucas-tests-tool-calling',\n",
       "  'A file was created at: ./lucas-tests-tool-calling/test1.txt',\n",
       "  'Hello Lucas! You just made a tool call!'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_call(\"Read the file contents from  ./lucas-tests-tool-calling/test1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T13:14:37.716776Z', 'done': True, 'done_reason': 'stop', 'total_duration': 598298875, 'load_duration': 27662208, 'prompt_eval_count': 568, 'prompt_eval_duration': 106904125, 'eval_count': 35, 'eval_duration': 461628500, 'model_name': 'llama3.2'} id='run-54d83e58-70bf-43f9-a640-660004933c23-0' tool_calls=[{'name': 'create_dir', 'args': {'folder_path': 'testing-multiple-calls'}, 'id': '23c63ace-fa05-4299-8359-10f79b76ff66', 'type': 'tool_call'}] usage_metadata={'input_tokens': 568, 'output_tokens': 35, 'total_tokens': 603}\n",
      "content='' additional_kwargs={} response_metadata={} id='run-037cca8a-afb0-44da-ac31-a103aa703d55-0' tool_calls=[{'name': 'create_dir', 'args': {'folder_path': 'testing-multiple-calls'}, 'id': 'b79a06a3-6805-40b7-b7d0-dd54a185b217', 'type': 'tool_call'}]\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T13:14:38.78166Z', 'done': True, 'done_reason': 'stop', 'total_duration': 322427417, 'load_duration': 11433083, 'prompt_eval_count': 600, 'prompt_eval_duration': 57377500, 'eval_count': 26, 'eval_duration': 252834334, 'model_name': 'llama3.2'} id='run-af6bd8eb-3b90-4ed7-b401-f3bf085d5f2a-0' tool_calls=[{'name': 'create_file', 'args': {'file_path': 'testing-multiple-calls/nested-file.txt'}, 'id': '51f1f6ab-70bf-4330-a2a0-96024e03c24f', 'type': 'tool_call'}] usage_metadata={'input_tokens': 600, 'output_tokens': 26, 'total_tokens': 626}\n",
      "Breaking after 3 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def agent_loop(query):\n",
    "    iter_count = 0\n",
    "    obs = []\n",
    "    acts_taken = []\n",
    "    while True:\n",
    "        output,obs,acts_taken = llm_call(query, obs, acts_taken)\n",
    "        print(output)\n",
    "        iter_count+=1\n",
    "        if iter_count>=3:\n",
    "            print(f\"Breaking after {iter_count} iterations\")\n",
    "            break\n",
    "        if output.content!=\"\":\n",
    "            break\n",
    "    \n",
    "    return output.content\n",
    "\n",
    "\n",
    "agent_loop(\"Create a folder in current directory named 'testing-multiple-calls' and inside that folder create a file named nested-file.txt\")        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-agents",
   "language": "python",
   "name": "oreilly-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
