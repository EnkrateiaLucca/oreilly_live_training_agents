{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6515816b",
   "metadata": {},
   "source": [
    "# Local RAG with LLaMA 3.2: Building an Adaptive System\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, we'll build an advanced Retrieval Augmented Generation (RAG) system using local models. Our system will incorporate several key features:\n",
    "- Adaptive routing between vector store and web search\n",
    "- Self-correction and hallucination detection\n",
    "- Document relevance grading\n",
    "\n",
    "## Setting Up Our Environment\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27597910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "# Initialize our local LLM\n",
    "llm = ChatOllama(model=\"llama3.2:3b-instruct-fp16\", temperature=0)\n",
    "llm_json = ChatOllama(model=\"llama3.2:3b-instruct-fp16\", temperature=0, format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525f7e1",
   "metadata": {},
   "source": [
    "## Creating Our Vector Store\n",
    "\n",
    "Let's set up a vector store with some sample documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ba8baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts: 100%|██████████| 6/6 [00:00<00:00, 15.76inputs/s]\n"
     ]
    }
   ],
   "source": [
    "# Sample URLs to load\n",
    "urls = [\n",
    "    \"https://langchain-ai.github.io/langgraph/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/low_level/\"\n",
    "]\n",
    "\n",
    "# Load and process documents\n",
    "loader = WebBaseLoader(urls[0])  # Loading first URL as example\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = splitter.split_documents(documents)\n",
    "\n",
    "# Create vector store\n",
    "embeddings = NomicEmbeddings(\n",
    "    model=\"nomic-embed-text-v1.5\", \n",
    "    inference_mode=\"local\"\n",
    ")\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfcebbd",
   "metadata": {},
   "source": [
    "## Building the Router Component\n",
    "\n",
    "We'll create a router to decide between vector store and web search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b775601a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are transformer neural networks?\n",
      "Routing: vectorstore\n",
      "\n",
      "Question: Who won the latest Super Bowl?\n",
      "Routing: websearch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "router_system_prompt = \"\"\"You are an expert at routing questions.\n",
    "Route to vectorstore for AI/ML topics and to web search for current events.\n",
    "Return JSON with key 'datasource' as either 'websearch' or 'vectorstore'.\"\"\"\n",
    "\n",
    "def route_question(question):\n",
    "    response = llm_json.invoke([\n",
    "        SystemMessage(content=router_system_prompt),\n",
    "        HumanMessage(content=question)\n",
    "    ])\n",
    "    return json.loads(response.content)\n",
    "\n",
    "# Test the router\n",
    "test_questions = [\n",
    "    \"What are transformer neural networks?\",\n",
    "    \"Who won the latest Super Bowl?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    result = route_question(question)\n",
    "    print(f\"Question: {question}\\nRouting: {result['datasource']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bcc2ab",
   "metadata": {},
   "source": [
    "## Implementing Document Grading\n",
    "\n",
    "Let's create a component to grade document relevance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058d4709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document relevance grade: {'relevant': True, 'explanation': \"The document provides a general overview of what transformers are, which is directly related to the question about how they work. However, it does not provide specific details on the inner workings or mechanisms of transformer networks, so while it's relevant in a broad sense, it may not be sufficient for someone looking for a detailed explanation.\"}\n"
     ]
    }
   ],
   "source": [
    "grader_prompt = \"\"\"You are grading document relevance.\n",
    "Rate if the document answers the question.\n",
    "Return JSON with 'relevant': true/false and 'explanation'.\"\"\"\n",
    "\n",
    "def grade_document(document, question):\n",
    "    prompt = f\"\"\"Document: {document}\n",
    "    Question: {question}\n",
    "    Is this document relevant?\"\"\"\n",
    "    \n",
    "    response = llm_json.invoke([\n",
    "        SystemMessage(content=grader_prompt),\n",
    "        HumanMessage(content=prompt)\n",
    "    ])\n",
    "    return json.loads(response.content)\n",
    "\n",
    "# Test document grading\n",
    "sample_doc = \"Transformers are a type of neural network architecture...\"\n",
    "test_question = \"How do transformer networks work?\"\n",
    "grade = grade_document(sample_doc, test_question)\n",
    "print(f\"Document relevance grade: {grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3570d",
   "metadata": {},
   "source": [
    "## Building the Answer Generator\n",
    "\n",
    "Finally, let's implement our RAG answer generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b30d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: The key innovation in transformer networks is the self-attention mechanism, which replaces traditional recurrent neural network (RNN) or convolutional neural network (CNN) architectures by allowing the model to attend to all positions in an input sequence simultaneously and weigh their importance.\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(context, question):\n",
    "    prompt = f\"\"\"Using this context: {context}\n",
    "    Answer this question: {question}\n",
    "    Keep the answer concise and accurate.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=prompt),\n",
    "        HumanMessage(content=question)\n",
    "    ])\n",
    "    return response.content\n",
    "\n",
    "# Test answer generation\n",
    "context = \"Transformers use self-attention mechanisms...\"\n",
    "question = \"What is the key innovation in transformer networks?\"\n",
    "answer = generate_answer(context, question)\n",
    "print(f\"Generated answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6db04f",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "Here's how to use all components together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6773cbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts: 100%|██████████| 1/1 [00:00<00:00, 32.88inputs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final answer: The main components of LangGraph are:\n",
      "\n",
      "1. **LangGraph Platform**: A commercial solution for deploying agentic applications in production, built on the open-source LangGraph framework.\n",
      "2. **LangGraph Server (APIs)**: Provides multiple streaming modes optimized for various application needs.\n",
      "3. **LangGraph SDKs (clients for the APIs)**: Clients for interacting with the LangGraph Server.\n",
      "4. **LangGraph CLI (command line tool for building the server)**: A command-line tool for building and managing the LangGraph Server.\n",
      "5. **LangGraph Studio (UI/debugger)**: A user interface for debugging and monitoring LangGraph applications.\n",
      "\n",
      "Additionally, LangGraph consists of:\n",
      "\n",
      "1. **Nodes**: The basic building blocks of a LangGraph graph, which can represent various components such as agents, tools, and state management.\n",
      "2. **Edges**: Connections between nodes that define the flow of data and control within the graph.\n",
      "3. **State**: The memory of the application, which can be persisted across runs using checkpointing mechanisms.\n",
      "\n",
      "These components work together to enable the creation, deployment, and management of agentic applications with LangGraph.\n"
     ]
    }
   ],
   "source": [
    "def process_query(question):\n",
    "    # 1. Route the question\n",
    "    route = route_question(question)\n",
    "    \n",
    "    # 2. Get relevant documents\n",
    "    if route['datasource'] == 'vectorstore':\n",
    "        docs = retriever.invoke(question)\n",
    "    else:\n",
    "        print(\"Would perform web search here\")\n",
    "        return\n",
    "        \n",
    "    # 3. Grade documents\n",
    "    relevant_docs = []\n",
    "    for doc in docs:\n",
    "        grade = grade_document(doc.page_content, question)\n",
    "        if grade['relevant']:\n",
    "            relevant_docs.append(doc)\n",
    "            \n",
    "    # 4. Generate answer\n",
    "    if relevant_docs:\n",
    "        context = \"\\n\".join(d.page_content for d in relevant_docs)\n",
    "        answer = generate_answer(context, question)\n",
    "        return answer\n",
    "    else:\n",
    "        return \"No relevant documents found\"\n",
    "\n",
    "# Test the complete system\n",
    "test_question = \"What are the main components LangGraph?\"\n",
    "result = process_query(test_question)\n",
    "print(f\"Final answer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c084b",
   "metadata": {},
   "source": [
    "This tutorial has shown how to build a local RAG system with adaptive routing, document grading, and answer generation. The system can intelligently choose between different data sources and verify the relevance of retrieved information before generating answers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-agents",
   "language": "python",
   "name": "oreilly-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
