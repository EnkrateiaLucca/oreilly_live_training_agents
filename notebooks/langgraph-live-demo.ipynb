{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1bf2cc3",
      "metadata": {},
      "source": [
        "# AV: can we put a man in the middle there so the workflow to route to one node or another based on what LLM understands from the user input?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "26331ffc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'> ## Documentation Index\\n> Fetch the complete documentation index at: https://docs.langchain.com/llm'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"./langgraph-thinking.md\", \"r\") as f: \n",
        "    input_text = f.read()\n",
        "\n",
        "input_text[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "6d5f57b6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hi — how can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 7, 'total_tokens': 89, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D7mPzm0iZPHUXfpVBjdpi03Jju0TB', 'finish_reason': 'stop', 'logprobs': None}, id='run-1313e4e7-a093-45bc-9eaa-33dd3f17d8d8-0', usage_metadata={'input_tokens': 7, 'output_tokens': 82, 'total_tokens': 89, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, MessagesState\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"gpt-5-mini\", model_provider=\"openai\")\n",
        "\n",
        "llm.invoke('hi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "43a4488f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RouterOutput(route='extract')"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class RouterOutput(BaseModel):\n",
        "    route: str = Field(description=\"The route to take based on the user's input, options: summarize, extract\")\n",
        "\n",
        "llm_router = llm.with_structured_output(RouterOutput)\n",
        "\n",
        "llm_router.invoke('extract info from this paper')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "439b66a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def user_input_router_node(state: CustomState):\n",
        "    \"\"\"Node: gets user input, runs LLM router, and stores the chosen route in state.\n",
        "    Nodes must return a state update (dict), not the next node name.\n",
        "    No need to pass through input_text -- StateGraph preserves fields automatically.\"\"\"\n",
        "    user_input = input(\"What do you want to do with the doc? \")\n",
        "    llm_output = llm_router.invoke(user_input)\n",
        "    if llm_output.route == 'summarize':\n",
        "        next_route = \"summarize_node\"\n",
        "    elif llm_output.route == 'extract':\n",
        "        next_route = \"extract_node\"\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid route: {llm_output.route}\")\n",
        "    return {\"next_route\": next_route}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "bbae528c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "llm = init_chat_model(\"gpt-5-mini\",\n",
        "                                 model_provider=\"openai\",\n",
        "                                 )\n",
        "\n",
        "\n",
        "def summarize_node(state: CustomState):\n",
        "    # input_text is a str from f.read(); wrap it in a HumanMessage for the LLM\n",
        "    raw_input_text = state[\"input_text\"]\n",
        "    summary = llm.invoke(\n",
        "        [\n",
        "            SystemMessage(content=\"You summarize text in bullet points covering the main information.\"),\n",
        "            HumanMessage(content=raw_input_text),\n",
        "        ]\n",
        "    )\n",
        "    return {\"messages\": [summary]}\n",
        "\n",
        "def extract_node(state: CustomState):\n",
        "    \"\"\"\n",
        "    Node: extracts structured information from the document using a custom Pydantic model and LangGraph.\n",
        "    Returns: a state update with extracted info as a list of dicts in 'messages'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define your custom Pydantic output schema for extraction\n",
        "    class ExtractedInfo(BaseModel):\n",
        "        title: str = Field(description=\"Title of the document\")\n",
        "        authors: list[str] = Field(description=\"List of authors\")\n",
        "        publication_year: int = Field(description=\"Year of publication\")\n",
        "        main_findings: str = Field(description=\"Main findings of the paper\")\n",
        "\n",
        "    # Wrap the LLM with the structured output schema\n",
        "    llm_extractor = llm.with_structured_output(ExtractedInfo)\n",
        "\n",
        "    raw_input_text = state[\"input_text\"]\n",
        "    # Call extraction LLM\n",
        "    extracted = llm_extractor.invoke(raw_input_text)\n",
        "    extracted_info_str = f\"Title: {extracted.title}\\nAuthors: {', '.join(extracted.authors)}\\nPublication Year: {extracted.publication_year}\\nMain Findings: {extracted.main_findings}\"\n",
        "    # You may choose to add to messages or a new field\n",
        "    return {\"messages\": [extracted_info_str]}\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "e0f418b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_after_user_input(state: dict):\n",
        "    \"\"\"Router: reads the route from state and returns the next node name.\n",
        "    Used by add_conditional_edges; only routing functions return node names.\"\"\"\n",
        "    return state[\"next_route\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "f66aea0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "from langgraph.graph import START, END\n",
        "\n",
        "class CustomState(MessagesState):\n",
        "    input_text: str\n",
        "\n",
        "builder = StateGraph(CustomState)\n",
        "\n",
        "builder.add_node(\"user_input\", user_input_router_node)\n",
        "builder.add_node(\"summarize_node\", summarize_node)\n",
        "builder.add_node(\"extract_node\", extract_node)\n",
        "builder.add_conditional_edges(\"user_input\", route_after_user_input)\n",
        "builder.add_edge(START, \"user_input\")\n",
        "# Router must be a function that takes state and returns next node name (not the node itself)\n",
        "builder.add_edge(\"summarize_node\", END)\n",
        "builder.add_edge(\"extract_node\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "output = graph.invoke({\"input_text\": input_text})\n",
        "\n",
        "# final output summary\n",
        "# Markdown(output[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "e3f84c97",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Title: Thinking in LangGraph\n",
              "Authors: LangChain Documentation\n",
              "Publication Year: 2024\n",
              "Main Findings: This guide explains how to design and build durable, inspectable AI agents with LangGraph by decomposing workflows into discrete nodes, defining shared raw state, and wiring nodes with explicit routing via Command objects. Key recommendations: break processes into small nodes (LLM, data, action, and human-input types), store only raw data in state and format prompts on-demand, handle errors with appropriate strategies (retry for transient, interrupt for user-fixable, bubble up unexpected), use interrupt() for human-in-the-loop pauses with persistent checkpoints, and add retry policies and checkpoints to nodes that call external services. The walkthrough implements a customer-support-email agent (read email, classify intent, search docs, bug tracking, draft reply, human review, send reply) with example node code, state schema, and graph compilation guidance."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(output[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "91abe38b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# output = graph.invoke({\"input_text\": input_text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "8ad0de2c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "- Purpose: walkthrough for designing a customer-support email agent with LangGraph using a node/state-based workflow.\n",
              "\n",
              "- High-level workflow (nodes):\n",
              "  - Read Email\n",
              "  - Classify Intent (intent, urgency, topic, summary)\n",
              "  - Doc Search (knowledge base)\n",
              "  - Bug Track (create/update issues)\n",
              "  - Draft Reply\n",
              "  - Human Review (interrupt/resume)\n",
              "  - Send Reply\n",
              "\n",
              "- Example scenarios the agent should handle:\n",
              "  - Password reset question, bug reports, urgent billing, feature requests, complex technical issues.\n",
              "\n",
              "- Design approach (five steps):\n",
              "  1. Map the process into discrete nodes and possible transitions.\n",
              "  2. Identify each node’s role: LLM steps, data steps, action steps, user-input steps.\n",
              "  3. Design shared state (store raw data only; format prompts on-demand).\n",
              "  4. Implement nodes as functions that take state and return updates (use Command to route).\n",
              "  5. Wire nodes into a StateGraph, compile, and run (use a checkpointer to persist state for interrupts).\n",
              "\n",
              "- State design guidance:\n",
              "  - Store raw, minimal data needed across steps (do not store formatted prompts).\n",
              "  - Example state structure (EmailAgentState):\n",
              "    - email_content, sender_email, email_id\n",
              "    - classification (EmailClassification dict or None)\n",
              "    - search_results (list[str] or None)\n",
              "    - customer_history (dict or None)\n",
              "    - draft_response (str or None)\n",
              "    - messages (list[str] or None)\n",
              "  - EmailClassification fields: intent (question, bug, billing, feature, complex), urgency (low/medium/high/critical), topic, summary.\n",
              "\n",
              "- Node implementation patterns:\n",
              "  - Nodes are small functions: read_email, classify_intent, search_documentation, bug_tracking, draft_response, human_review, send_reply.\n",
              "  - Use LLM steps for classification and drafting; use structured output when possible.\n",
              "  - Nodes decide routing and return Command(update=..., goto=\"next_node\").\n",
              "  - Format prompts at call-time using raw state.\n",
              "\n",
              "- classify_intent specifics:\n",
              "  - Use structured LLM output (EmailClassification).\n",
              "  - Route based on intent/urgency: e.g., billing or critical → human_review; question/feature → search_documentation; bug → bug_tracking.\n",
              "\n",
              "- search_documentation and bug_tracking:\n",
              "  - Store raw search results or ticket IDs in state.\n",
              "  - Add retry logic for transient search errors and handle recoverable errors by storing error info.\n",
              "\n",
              "- draft_response:\n",
              "  - Build prompt from classification, search results, and customer history formatted on-demand.\n",
              "  - Generate response with LLM and decide if human review is needed (high/critical urgency or complex intent).\n",
              "\n",
              "- human_review:\n",
              "  - Use interrupt() to pause for human input; interrupt must be the first action in the node (code before it will re-run on resume).\n",
              "  - Resume with human decision: approved → send_reply; rejected → END (human handles).\n",
              "\n",
              "- send_reply:\n",
              "  - Perform final action (call email service); unexpected exceptions should bubble up.\n",
              "\n",
              "- Error handling strategies:\n",
              "  - Transient errors: system retries (exponential backoff).\n",
              "  - LLM-recoverable errors: capture in state and loop back so LLM can adjust.\n",
              "  - User-fixable errors: pause with interrupt() and request input.\n",
              "  - Unexpected errors: let them bubble up for developer debugging.\n",
              "\n",
              "- Durable execution and persistence:\n",
              "  - Use a checkpointer (e.g., MemorySaver) for interrupts and long-running flows.\n",
              "  - Provide thread_id in config to group conversation state.\n",
              "  - LangGraph checkpoints at node boundaries (async durability by default); durability modes can be tuned (exit/sync).\n",
              "\n",
              "- Graph wiring:\n",
              "  - Minimal explicit edges needed because nodes handle routing with Command objects.\n",
              "  - Example edges: START→read_email, read_email→classify_intent, send_reply→END.\n",
              "\n",
              "- Testing / run flow:\n",
              "  - Invoke the compiled app with initial state and config (thread_id).\n",
              "  - App pauses at interrupt and returns interrupt token.\n",
              "  - Resume by invoking with a Command containing resume payload (approved/edited_response).\n",
              "\n",
              "- Design trade-offs:\n",
              "  - Smaller nodes = finer checkpoints, better observability, easier retries/testing, clearer failure isolation.\n",
              "  - Larger nodes may be simpler but risk redoing more work on failure and reduce observability.\n",
              "\n",
              "- Key principles & tips:\n",
              "  - Break tasks into single-purpose nodes.\n",
              "  - Keep state raw and format when needed.\n",
              "  - Make routing explicit via Command return types.\n",
              "  - Treat human-in-the-loop as first-class via interrupt/resume.\n",
              "\n",
              "- Next steps / extensions:\n",
              "  - Human-in-the-loop patterns (batch approvals, tool approvals).\n",
              "  - Subgraphs for complex tasks.\n",
              "  - Streaming for real-time progress.\n",
              "  - Observability (LangSmith) and logging/monitoring.\n",
              "  - Integrate more tools (web search, databases, APIs).\n",
              "  - Implement retry/caching strategies per-node as needed.\n",
              "\n",
              "- Summary insight:\n",
              "  - LangGraph encourages decomposition into nodes, shared raw state, explicit routing via commands, durable execution with checkpoints, and clear handling of errors and human interventions."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(output[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6650d335",
      "metadata": {},
      "source": [
        "# can i have multiple workflows, and the agent to decide which workflow to start? what happens if the human in the loop (got the nomenclature right now) asks a question that is not in context and tries to jump to the other workflow midway? or asks something random? how we can deal with that?"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
